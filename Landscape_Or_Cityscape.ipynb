{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landscape or Cityscape?\n",
    "\n",
    "The idea behind this notebook is to test out various CNNs to determine which networks are best suited to determining if an picture is an image of a city or an image of nature. My data sources for this experiment are the subreddits r/earthporn and r/cityporn. I chose these subreddits as they are focused and fairly strictly moderated. Each contains only aestetically pleasing pictures of landscapes and cityscapes respectively. I took all the directly linked images from these subreddits since 1/1/17 that have over a certain threshold of upvotes, which gave me a few thousand images that are almost entirely either a picture of a natural landscape or a picture of a cityscape\n",
    "\n",
    "I will use keras' implementation of VGG16 as feature extration for a few kinds of classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#general imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import keras\n",
    "%matplotlib inline\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#keras specific imports\n",
    "from keras import Model, Input\n",
    "from keras.layers import Dense, Flatten,GlobalMaxPool2D\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.applications import VGG16,vgg16\n",
    "from keras.applications import ResNet50, resnet50\n",
    "from keras.applications import InceptionV3, inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from project_utils import get_test, create_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'project_utils' from '/home/ubuntu/Landscape_or_Cityscape/project_utils.py'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import project_utils\n",
    "reload(project_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading pretrained model weights\n",
    "VGG = VGG16(weights = 'imagenet', include_top = False, input_shape= (224,224,3))\n",
    "\n",
    "RN50 = ResNet50(weights = 'imagenet', include_top = False, input_shape= (224,224,3))\n",
    "\n",
    "IV3 = InceptionV3(weights = 'imagenet', include_top = False, input_shape= (224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function takes in headless model (some pretrained CNN)\n",
    "# and adds a max pooling and two dense fully connected layers to it.\n",
    "def create_model(base_layers):\n",
    "    for layer in base_layers.layers:\n",
    "        layer.trainable = False\n",
    "    X = base_layers.output\n",
    "    X = GlobalMaxPool2D()(X)\n",
    "    X = Dense(200, activation = 'relu', name = '1')(X)\n",
    "    X = Dense(100, activation = 'relu',name = '2')(X)\n",
    "    predictions = Dense(2 , activation = 'softmax',name = '3')(X)\n",
    "    model = Model(inputs = base_layers.inputs, outputs = predictions)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4850 images belonging to 2 classes.\n",
      "Found 605 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#Preparing data generators\n",
    "#this keras utilitiy will continously generate images that have been slightly rotated \n",
    "#or shifted or flipped from the directories where I stored the images for both training and validation\n",
    "data_gen = ImageDataGenerator(shear_range= 0.2,\n",
    "                                    zoom_range= 0.2,\n",
    "                                    horizontal_flip= True,\n",
    "                                    width_shift_range=0.2,\n",
    "                                    height_shift_range = 0.2,\n",
    "                                    preprocessing_function= (lambda x: x/127.5 - 1))                                           \n",
    "\n",
    "train_generator = data_gen.flow_from_directory('./images/train', target_size= (224,224), class_mode= 'categorical')\n",
    "validation_generator = data_gen.flow_from_directory('./images/validation', target_size= (224,224), class_mode= 'categorical')\n",
    "\n",
    "vgg_model = create_model(VGG)\n",
    "\n",
    "vgg_model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this will cause the model to stop fitting if the validation loss has gone down for 3 epochs\n",
    "#this is to prevent overfitting by training the model for too long\n",
    "early_stopping = EarlyStopping(min_delta = .01, \n",
    "                               patience = 3,\n",
    "                               verbose = 1,\n",
    "                               mode = 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "18/20 [==========================>...] - ETA: 21s - loss: 0.6063 - acc: 0.8003"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py:2438: DecompressionBombWarning: Image size (93153600 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "19/20 [===========================>..] - ETA: 10s - loss: 0.5882 - acc: 0.8059"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/PIL/Image.py:2438: DecompressionBombWarning: Image size (98130452 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 320s 16s/step - loss: 0.5924 - acc: 0.8047 - val_loss: 1.0087 - val_acc: 0.4437\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 278s 14s/step - loss: 0.3160 - acc: 0.8750 - val_loss: 0.2677 - val_acc: 0.9117\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 296s 15s/step - loss: 0.2581 - acc: 0.9219 - val_loss: 0.1946 - val_acc: 0.9500\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 273s 14s/step - loss: 0.2768 - acc: 0.9077 - val_loss: 0.3237 - val_acc: 0.8896\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 296s 15s/step - loss: 0.3042 - acc: 0.8984 - val_loss: 0.3158 - val_acc: 0.8969\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 281s 14s/step - loss: 0.2378 - acc: 0.9094 - val_loss: 0.1783 - val_acc: 0.9369\n",
      "Epoch 7/20\n",
      "19/20 [===========================>..] - ETA: 9s - loss: 0.2032 - acc: 0.9424 "
     ]
    }
   ],
   "source": [
    "vgg_model.fit_generator(generator = train_generator, \n",
    "                        epochs=20,\n",
    "                        steps_per_epoch= 20, #there are 32 images per step so this is 64 per epoch\n",
    "                        callbacks = [early_stopping],\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps=10) #validation on 320 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes and evalutes the model on the test set\n",
    "X_test, y_test = project_utils.get_test('_3_10','vgg16')\n",
    "\n",
    "vgg_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a confusion matrix for the test set\n",
    "preds = vgg_model.predict(X_test)\n",
    "\n",
    "conf_matrix = create_confusion_matrix(y_test,preds,{0:'city',1:'earth'})\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to help with error analysis and sanity checking\n",
    "def show_and_predict(path,model,modelname):\n",
    "    pylab.imshow(pylab.imread(path))\n",
    "    prepped = project_utils.prep_for_model(path,modelname)\n",
    "    pred = model.predict(prepped)\n",
    "    print(\"p(city): {}, p(nature): {}\".format(pred[0,0],pred[0,1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict('images/test/cityporn_3_10/image229.jpg',vgg_model,'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict('images/test/earthporn_3_10/image3077.jpg',vgg_model,'vgg16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict('images/test/cityporn_3_10/image971.jpg',vgg_model,'vgg16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This network is doing okay, but potentially we can do even better! Now to try out some other base networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iv3_model = create_model(IV3)\n",
    "\n",
    "iv3_model.compile(optimizer='rmsprop',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "vgg_model.fit_generator(generator = train_generator, \n",
    "                        epochs=20,\n",
    "                        steps_per_epoch= 20, #there are 32 images per step so this is 64 per epoch\n",
    "                        callbacks = [early_stopping],\n",
    "                        validation_data = validation_generator,\n",
    "                        validation_steps=10) #validation on 320 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, y_test = project_utils.get_test('_3_10','inception_v3')\n",
    "\n",
    "iv3_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes a confusion matrix for the test set\n",
    "preds = iv3_model.predict(X_test)\n",
    "\n",
    "conf_matrix = create_confusion_matrix(y_test,preds,{0:'city',1:'earth'})\n",
    "\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_and_predict('images/test/cityporn_3_10/image229.jpg',iv3_model,'inception_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_and_predict('images/test/earthporn_3_10/image3077.jpg',iv3_model,'inception_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_and_predict('images/test/cityporn_3_10/image971.jpg',iv3_model,'inception_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
